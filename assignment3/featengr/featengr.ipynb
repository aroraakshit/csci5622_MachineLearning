{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Homework \n",
    "***\n",
    "**Name**: Akshit Arora\n",
    "\n",
    "**Kaggle Username**: akshitarora\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 23rd**. Additionally, you must make at least one submission to the **Kaggle** competition before it closes at **4:59pm on Friday February 23rd**. Submit only this Jupyter notebook to Moodle. Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "When people are discussing popular media, there’s a concept of spoilers. That is, critical information about the plot of a TV show, book, or movie that “ruins” the experience for people who haven’t read / seen it yet.\n",
    "\n",
    "The goal of this assignment is to do text classification on forum posts from the website [tvtropes.org](http://tvtropes.org/), to predict whether a post is a spoiler or not. We'll be using the logistic regression classifier provided by sklearn.\n",
    "\n",
    "Unlike previous assignments, the code provided with this assignment has all of the functionality required. Your job is to make the functionality better by improving the features the code uses for text classification.\n",
    "\n",
    "**NOTE**: Because the goal of this assignment is feature engineering, not classification algorithms, you may not change the underlying algorithm or it's parameters\n",
    "\n",
    "This assignment is structured in a way that approximates how classification works in the real world: Features are typically underspecified (or not specified at all). You, the data digger, have to articulate the features you need. You then compete against others to provide useful predictions.\n",
    "\n",
    "It may seem straightforward, but do not start this at the last minute. There are often many things that go wrong in testing out features, and you'll want to make sure your features work well once you've found them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle In-Class Competition \n",
    "***\n",
    "\n",
    "In addition to turning in this notebook on Moodle, you'll also need to submit your predictions on Kaggle, an online tournament site for machine learning competitions. The competition page can be found here:  \n",
    "\n",
    "[https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018](https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018)\n",
    "\n",
    "Additionally, a private invite link for the competition has been posted to Piazza. \n",
    "\n",
    "The starter code below has a `model_predict` method which produces a two column CSV file that is correctly formatted for Kaggle (predictions.csv). It should have the example Id as the first column and the prediction (`True` or `False`) as the second column. If you change this format your submissions will be scored as zero accuracy on Kaggle. \n",
    "\n",
    "**Note**: You may only submit **THREE** predictions to Kaggle per day.  Instead of using the public leaderboard as your sole evaluation processes, it is highly recommended that you perform local evaluation using a validation set or cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 1: Feature Engineering \n",
    "***\n",
    "\n",
    "The `FeatEngr` class is where the magic happens.  In it's current form it will read in the training data and vectorize it using simple Bag-of-Words.  It then trains a model and makes predictions.  \n",
    "\n",
    "25 points of your grade will be generated from your performance on the the classification competition on Kaggle. The performance will be evaluated on accuracy on the held-out test set. Half of the test set is used to evaluate accuracy on the public leaderboard.  The other half of the test set is used to evaluate accuracy on the private leaderboard (which you will not be able to see until the close of the competition). \n",
    "\n",
    "You should be able to significantly improve on the baseline system (i.e. the predictions made by the starter code we've provided) as reported by the Kaggle system.  Additionally, the top **THREE** students from the **PRIVATE** leaderboard at the end of the contest will receive 5 extra credit points towards their Problem 1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#defining stemmer and stopwords list\n",
    "ps = PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "def stemmed_words(doc):\n",
    "    temp = (ps.stem(w) for w in analyzer(doc))\n",
    "    temp2 = [word for word in temp if word not in punctuation]\n",
    "    return temp2\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "stoplist_combined = set.union(stopwords_nltk_en, stopwords_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the imdb dataset for useful features (like genre), make train2.csv and test3.csv files with an additional column containing string for all genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dfimdb = pd.read_csv('title.basics.tsv', sep='\\t', header=0) #source: http://www.imdb.com/interfaces/\n",
    "dfTrain2 = pd.read_csv(\"train2.tsv\",sep='\\t') #preprocessed from comma separated to tab separated file\n",
    "dfTest3 = pd.read_csv(\"test3.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'GameOfThrones' -> 'Game Of Thrones'\n",
    "from re import finditer\n",
    "def camel_case_split(identifier):\n",
    "    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing and adding genre column to Training set\n",
    "gen_dict = {}\n",
    "import math\n",
    "for index,row in dfTrain2.iterrows():\n",
    "    temp_name = ' '.join(camel_case_split(row[\"page\"]))\n",
    "    if pd.isnull(row[\"genre\"]):\n",
    "        if temp_name in gen_dict:\n",
    "            dfTrain2.genre.iloc[index] = gen_dict[temp_name]\n",
    "        else:\n",
    "            temp_genre = \"unknown\"\n",
    "            if temp_name in dfimdb.values:\n",
    "                list_of_idx = dfimdb.index[dfimdb['primaryTitle'] == temp_name].tolist()\n",
    "                temp_genre_list = list([ str(dfimdb.iloc[i]['genres']) for i in list_of_idx])\n",
    "                temp_genre = ','.join(temp_genre_list)\n",
    "            \n",
    "            gen_dict[temp_name] = temp_genre\n",
    "            dfTrain2.genre.iloc[index] = gen_dict[temp_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting out repeated genres\n",
    "import math\n",
    "for index,row in dfTrain2.iterrows():\n",
    "    if(row[\"genre\"] != \"unknown\"):\n",
    "        if(type(row[\"genre\"]) == float):\n",
    "            dfTrain2.genre.iloc[index] = \"unknown\"\n",
    "        else:\n",
    "            dfTrain2.genre.iloc[index] = ' '.join(set(row[\"genre\"].split(',')))\n",
    "    print(dfTrain2.genre.iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving dfTrain to tab separated format!\n",
    "dfTrain2.to_csv('train2.tsv',sep='\\t',index=False)\n",
    "dfTrain2.to_csv('train2_2.tsv',sep='\\t',index=False) #backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making similar genre column for test set\n",
    "gen_dict2 = {}\n",
    "import math\n",
    "for index,row in dfTest3.iterrows():\n",
    "    temp_name = ' '.join(camel_case_split(row[\"page\"]))\n",
    "    if pd.isnull(row[\"genre\"]):\n",
    "        if temp_name in gen_dict2:\n",
    "            dfTest3.genre.iloc[index] = gen_dict2[temp_name]\n",
    "        else:\n",
    "            temp_genre = \"unknown\"\n",
    "            if temp_name in dfimdb.values:\n",
    "                list_of_idx = dfimdb.index[dfimdb['primaryTitle'] == temp_name].tolist()\n",
    "                temp_genre_list = list([ str(dfimdb.iloc[i]['genres']) for i in list_of_idx])\n",
    "                temp_genre = ','.join(temp_genre_list)\n",
    "                if(type(temp_genre) == float):\n",
    "                    temp_genre = \"unknown\"\n",
    "            if(temp_genre != \"unknown\"):\n",
    "                if(type(temp_genre) == float):\n",
    "                    temp_genre = \"unknown\"\n",
    "                else:\n",
    "                    temp_genre = ' '.join(set(temp_genre.split(',')))\n",
    "            \n",
    "            gen_dict2[temp_name] = temp_genre\n",
    "            dfTest3.genre.iloc[index] = gen_dict2[temp_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving test data\n",
    "dfTest3.to_csv('test3.tsv',sep='\\t',index=False)\n",
    "dfTest3.to_csv('test3_2.tsv',sep='\\t',index=False) #backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: Feature Union with Heterogeneous Data Sources: http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class XYZTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        import nltk\n",
    "        import sklearn\n",
    "        X = []\n",
    "        for ii, x in enumerate(examples):\n",
    "            temp = nltk.pos_tag(x)\n",
    "            app = []\n",
    "            for j in temp:\n",
    "                app.append(j[1])\n",
    "            X.append(' '.join(app))\n",
    "            \n",
    "        vect = sklearn.feature_extraction.text.TfidfVectorizer(analyzer='word')\n",
    "        gh = vect.fit_transform(X) \n",
    "        return gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "class FeatEngr:\n",
    "    def __init__(self):\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        import nltk\n",
    "        from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "        allmyfeatures = FeatureUnion(transformer_list=[\n",
    "            #apply tfidf on sentence\n",
    "                (\"tfidf\",Pipeline([\n",
    "                    ('selector',ItemSelector(key='sentence')),\n",
    "                    ('tfidf',sklearn.feature_extraction.text.TfidfVectorizer(analyzer = stemmed_words, ngram_range=(1, 1) ,use_idf=True, smooth_idf=True, sublinear_tf=False))\n",
    "               ])), \n",
    "            (\"tfidf2\",Pipeline([\n",
    "                    ('selector',ItemSelector(key='genre')),\n",
    "                    ('tfidfG',sklearn.feature_extraction.text.TfidfVectorizer(analyzer = 'word', ngram_range=(1,3),use_idf=True, smooth_idf=True, sublinear_tf=False)),\n",
    "                    ('best', TruncatedSVD(n_components=82))\n",
    "                ])), \n",
    "            #applying count vectorizer on genres\n",
    "            (\"cvG\", Pipeline([\n",
    "                ('selector',ItemSelector(key='genre')),\n",
    "                ('countVect', CountVectorizer(tokenizer=nltk.word_tokenize,analyzer='word'))\n",
    "            ])),\n",
    "            (\"cvGdd\", Pipeline([\n",
    "                ('selector',ItemSelector(key='sentence')),\n",
    "                ('countVect', XYZTransformer())\n",
    "            ])),\n",
    "            (\"cvT\", Pipeline([\n",
    "                ('selector',ItemSelector(key='trope')),\n",
    "                ('countVect2', CountVectorizer(analyzer='word',ngram_range=(1,1)))\n",
    "                ])),\n",
    "            (\"cv2\", Pipeline([\n",
    "                ('selector',ItemSelector(key='sentence')),\n",
    "                ('countVect3', sklearn.feature_extraction.text.HashingVectorizer (ngram_range=(1, 1),analyzer=stemmed_words,norm='l1',alternate_sign=False)),\n",
    "                ('best', TruncatedSVD(n_components=92))\n",
    "                ]))\n",
    "        ])\n",
    "        \n",
    "        self.vectorizer = allmyfeatures\n",
    "\n",
    "    def build_train_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.vectorizer.fit_transform(examples)\n",
    "\n",
    "    def get_test_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.vectorizer.transform(examples)\n",
    "\n",
    "    def show_top10(self):\n",
    "        \"\"\"\n",
    "        prints the top 10 features for the positive class and the \n",
    "        top 10 features for the negative class. \n",
    "        \"\"\"\n",
    "        feature_names = np.asarray(self.vectorizer.get_feature_names())\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-10:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:10]\n",
    "        print(\"Pos: %s\" % \" \".join(feature_names[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(feature_names[bottom10]))\n",
    "        \n",
    "    def model_predict(self):\n",
    "        \"\"\"\n",
    "        Method to read in test data from file, make predictions\n",
    "        using trained model, and dump results to file \n",
    "        \"\"\"\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        # read in test data \n",
    "        dfTest  = pd.read_csv('test3.tsv',sep='\\t')\n",
    "        \n",
    "        # featurize test data \n",
    "        #dfTest['sentence1'] = dfTest[['trope', 'sentence','genre']].apply(lambda x: ' '.join(x), axis=1)\n",
    "        self.X_test = self.get_test_features((dfTest[[\"sentence\",\"trope\",\"genre\",\"page\"]]))\n",
    "        \n",
    "        # make predictions on test data \n",
    "        pred = self.logreg.predict(self.X_test)\n",
    "        y_test = np.array(dfTest[\"spoiler\"], dtype=int)\n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        pd.DataFrame({\"spoiler\": np.array(pred, dtype=bool)}).to_csv(\"prediction.csv\", index=True, index_label=\"Id\")\n",
    "        print(\"Predictions generated.\")\n",
    "        acc_train = accuracy_score(y_test,pred)\n",
    "        print(\"accuracy on train set: \" + str(acc_train))\n",
    "    \n",
    "    def train_model_cv(self, random_state=None, train_split=0.8, cv=True):\n",
    "        \"\"\"\n",
    "        Method to read in training data from file, random splitting to cross validation-train set and \n",
    "        train Logistic Regression classifier on just the training set. \n",
    "        \n",
    "        :param random_state: seed for random number generator\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import random\n",
    "        \n",
    "        # load data \n",
    "        dfTraining = pd.read_csv(\"train2.tsv\",sep='\\t')\n",
    "        \n",
    "        #create random split into train and test set\n",
    "        msk = np.random.RandomState(random_state).rand(len(dfTraining)) < train_split\n",
    "        #dfTraining['sentence1'] = dfTraining[['trope', 'sentence','genre']].apply(lambda x: ' '.join(x), axis=1)\n",
    "        # get training features and labels \n",
    "        X_training = self.build_train_features(dfTraining[[\"sentence\",\"trope\",\"genre\",\"page\"]])\n",
    "        y_training = np.array(dfTraining[\"spoiler\"], dtype=int)\n",
    "        \n",
    "        # train logistic regression model.  !!You MAY NOT CHANGE THIS!! \n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        if(cv):\n",
    "            X_cv = X_training[~msk]\n",
    "            y_cv = y_training[~msk]\n",
    "            self.X_train = X_training[msk]\n",
    "            self.y_train = y_training[msk]\n",
    "            self.logreg.fit(self.X_train, self.y_train)\n",
    "            #get valid set features and labels\n",
    "            pred_cv = self.logreg.predict(X_cv)\n",
    "            acc_cv = accuracy_score(y_cv,pred_cv)\n",
    "            print(\"accuracy on cv set: \" + str(acc_cv))\n",
    "            pred_train = self.logreg.predict(self.X_train)\n",
    "            acc_train = accuracy_score(self.y_train,pred_train)\n",
    "            print(\"accuracy on train set: \" + str(acc_train))\n",
    "        else:\n",
    "            self.logreg.fit(X_training, y_training)\n",
    "            print(\"number of features: \"+str(X_training.todense().shape))\n",
    "            pred_train = self.logreg.predict(X_training)\n",
    "            acc_train = accuracy_score(y_training,pred_train)\n",
    "            print(\"accuracy on train set: \" + str(acc_train))\n",
    "#         plt = plot_learning_curve(self.logreg, \"Learning Curves for logreg\", X_training, y_training,ylim=(0.0,1.0))\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on cv set: 0.7613305613305613\n",
      "accuracy on train set: 0.9066387872451647\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the FeatEngr clas \n",
    "feat = FeatEngr()\n",
    "\n",
    "# Train your Logistic Regression classifier \n",
    "feat.train_model_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: (11970, 18099)\n",
      "accuracy on train set: 0.899749373433584\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 18097 features per sample; expecting 18099",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-9c03ed6bae84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfeat2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatEngr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeat2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1230\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeat2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-c88ee7eff4d1>\u001b[0m in \u001b[0;36mmodel_predict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# make predictions on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfTest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"spoiler\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# dump predictions to file for submission to Kaggle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 305\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 18097 features per sample; expecting 18099"
     ]
    }
   ],
   "source": [
    "# to produce kaggle submission file\n",
    "feat2 = FeatEngr()\n",
    "feat2.train_model_cv(random_state = 1230, cv=False)\n",
    "feat2.model_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Motivation and Analysis \n",
    "***\n",
    "\n",
    "The job of the written portion of the homework is to convince the grader that:\n",
    "\n",
    "- Your new features work\n",
    "- You understand what the new features are doing\n",
    "- You had a clear methodology for incorporating the new features\n",
    "\n",
    "Make sure that you have examples and quantitative evidence that your features are working well. Be sure to explain how you used the data (e.g., did you have a validation set? did you do cross-validation?) and how you inspected the results. In addition, it is very important that you show some kind of an **error analysis** throughout your process.  That is, you should demonstrate that you've looked at misclassified examples and put thought into how you can craft new features to improve your model. \n",
    "\n",
    "A sure way of getting a low grade is simply listing what you tried and reporting the Kaggle score for each. You are expected to pay more attention to what is going on with the data and take a data-driven approach to feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: For solving the problem above, I systematically tried including new features from the given dataset and after a while felt the need of adding additional information from internet about individual data points. Below is a detailed timeline of how I went about achieving the accuracy of predictions I submitted on kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check if the data is balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of spoiler examples in the whole set: 6288\n",
      "number of not spoiler examples in the whole set: 5682\n",
      "proportion of spoiler to not spoiler examples: 52.531328320802004 : 47.468671679197996\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "dfTrain = pd.read_csv(\"train.csv\")\n",
    "y_train = np.array(dfTrain[\"spoiler\"], dtype=int)\n",
    "spoiler = 0\n",
    "not_spoiler = 0\n",
    "for i in range(len(y_train)):\n",
    "    if(y_train[i]==1):\n",
    "        spoiler = spoiler+1\n",
    "    else:\n",
    "        not_spoiler = not_spoiler + 1\n",
    "print(\"number of spoiler examples in the whole set: \" + str(spoiler))\n",
    "print(\"number of not spoiler examples in the whole set: \" + str(not_spoiler))\n",
    "print(\"proportion of spoiler to not spoiler examples: \" + str(spoiler/(spoiler+not_spoiler)*100) + \" : \"+str(not_spoiler/(spoiler+not_spoiler)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is balanced since we have almost equal examples of both spoiler and not spoiler examples (52:47 ratio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using Bag-of-words on sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What I did?**: For encoding sentences to feed the logistic regression model, I first tried using bag-of-words (BoW) model for sentences. I used sklearn's CountVectorizer for this purpose. I implemented cross-validation by splitting the training data randomly into train and cross-validation (CV) set (80%-20% split, stays the same throughout my experiments) and ran every experiment around 10 times to get an idea of range of accuracy of training and test sets. I also removed stop words (including punctuations) and normalized the features before feeding it to the model.<br>\n",
    "**Why I did it?**: BoW allows us to count how many times a word appears in a document. Those word counts allow us to compare sentences and gauge their similarities. BoW measures frequencies.<br>\n",
    "**What went wrong?**: My training set accuracy was always above 98% but CV set was between 50-60% => implying overfitting. In order to handle this overfitting, I went on exploring other features that can be extracted from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using TF-IDF on sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What I did?**: Instead of using CountVectorizer I used sklearn's TfidfVectorizer.<br>\n",
    "**Why I did it?**: TF-IDF measures the number of times that words appear in a given document (that’s term frequency). But because words such as “and” or “the” appear frequently in all documents, those are systematically discounted. That’s the inverse-document frequency part. The more documents a word appears in, the less valuable that word is as a signal. That’s intended to leave only the frequent AND distinctive words as markers.<br>\n",
    "**Intuition**: Capturing relevance of words in a sentence instead of just the frequencies. <br>\n",
    "**Result?**: My training set accuracy went down to 82% and my CV accuracy went up to 67%. This indicates that overfitting has reduced and my model is now able to generalize better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using CountVectorizer on Tropes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What I did?**: In order to give more information to model about the data, I started looking at other columns provided in the dataset like tropes.<br>\n",
    "**Why I did it?**: There were 2 more columns: Trope, Page. I tried page, but the training accuracy -> 84% and CV accuracy-> 60%. But by using tropes, training set accuracy went up to 91% and CV accuracy went up to 75%. Therefore I chose tropes.<br>\n",
    "**Intuition**: The model was suffering from high bias since both training and test accuracies were down before. Therefore I started to look for new features.\n",
    "**Result?**: Training set accuracy 91% and CV accuracy 74%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Using CountVectorizer on Genres from IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What I did?**: Pre-processed training set to include genres from imdb dataset.<br>\n",
    "**Intuition**: Genres like Talk Shows, Game Shows, Reality-TV will naturally not have as many spoilers as Thriller, Drama, Crime, Sci-fi. Therefore, such information will help classify the sentence for whether it will have a spoiler or not. <br>\n",
    "**Result?**: Training set accuracy 90% and CV accuracy 74%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Using TF IDF on Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What I did?**: Capturing only relevant genres instead of all of them.<br>\n",
    "**Intuition**: I observed that there were multiple rows corresponding to the same primary title (name of the movie/tvshow) maybe because of different seasons of the show / differen version. Hence I consider only the genres that are relevant. <br>\n",
    "**Result?**: Training set accuracy 91% and CV accuracy 75%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What I did?**: Capturing the features that matter the most using sklearn's truncated SVD.<br>\n",
    "**Intuition**: I had 1067472 features by now and not all of them mattered as much.<br>\n",
    "**Result?**: Dimensions reduced to: 18074. Training set accuracy 91% and CV accuracy 77%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, I have used code from couple of websites like scikit-learn's official website, IMDB website etc. The source URL's have been mentioned as comments throughout the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints \n",
    "***\n",
    "\n",
    "- Don't use all the data until you're ready. \n",
    "\n",
    "- Examine the features that are being used.\n",
    "\n",
    "- Do error analyses.\n",
    "\n",
    "- If you have questions that aren’t answered in this list, feel free to ask them on Piazza.\n",
    "\n",
    "### FAQs \n",
    "***\n",
    "\n",
    "> Can I heavily modify the FeatEngr class? \n",
    "\n",
    "Totally.  This was just a starting point.  The only thing you cannot modify is the LogisticRegression classifier.  \n",
    "\n",
    "> Can I look at TV Tropes?\n",
    "\n",
    "In order to gain insight about the data yes, however, your feature extraction cannot use any additional data (beyond what I've given you) from the TV Tropes webpage.\n",
    "\n",
    "> Can I use IMDB, Wikipedia, or a dictionary?\n",
    "\n",
    "Yes, but you are not required to. So long as your features are fully automated, they can use any dataset other than TV Tropes. Be careful, however, that your dataset does not somehow include TV Tropes (e.g. using all webpages indexed by Google will likely include TV Tropes).\n",
    "\n",
    "> Can I combine features?\n",
    "\n",
    "Yes, and you probably should. This will likely be quite effective.\n",
    "\n",
    "> Can I use Mechanical Turk?\n",
    "\n",
    "That is not fully automatic, so no. You should be able to run your feature extraction without any human intervention. If you want to collect data from Mechanical Turk to train a classifier that you can then use to generate your features, that is fine. (But that’s way too much work for this assignment.)\n",
    "\n",
    "> Can I use a Neural Network to automatically generate derived features? \n",
    "\n",
    "No. This assignment is about your ability to extract meaningful features from the data using your own experimentation and experience.\n",
    "\n",
    "> What sort of improvement is “good” or “enough”?\n",
    "\n",
    "If you have 10-15% improvement over the baseline (on the Public Leaderboard) with your features, that’s more than sufficient. If you fail to get that improvement but have tried reasonable features, that satisfies the requirements of assignment. However, the extra credit for “winning” the class competition depends on the performance of other students.\n",
    "\n",
    "> Where do I start?  \n",
    "\n",
    "It might be a good idea to look at the in-class notebook associated with the Feature Engineering lecture where we did similar experiments. \n",
    "\n",
    "\n",
    "> Can I use late days on this assignment? \n",
    "\n",
    "You can use late days for the write-up submission, but the Kaggle competition closes at **4:59pm on Friday February 23rd**\n",
    "\n",
    "> Why does it say that the competition ends at 11:59pm when the assignment says 4:59pm? \n",
    "\n",
    "The end time/date are in UTC.  11:59pm UTC is equivalent to 4:59pm MST.  Kaggle In-Class does not allow us to change this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
