{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Naive Bayes\n",
    "***\n",
    "\n",
    "<img src=\"figs/bayes.jpg\" width=1201 height=50> \n",
    "\n",
    "<!---\n",
    "![my_image](files/figs/bayes.jpg)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prob1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prob2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Naive Bayes on Symbols\n",
    "***\n",
    "\n",
    "> This problem was adopted from [Naive Bayes and Text Classification I: Introduction and Theory](https://arxiv.org/abs/1410.5329) by Sebastian Raschka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following training set of 12 symbols which have been labeled as either + or -: \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figs/shapes.png\" width=500>\n",
    "\n",
    "\n",
    "Answer the following questions: \n",
    "\n",
    "\n",
    "**A**: What are the general features associated with each training example? Shape and Color <br>\n",
    "labels are + and -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we'll use Naive Bayes to classify the following test example: \n",
    "\n",
    "<img src=\"figs/bluesquare.png\" width=200>\n",
    "\n",
    "OK, so this symbol actually appears in the training set, but let's pretend that it doesn't.  \n",
    "\n",
    "The decision rule can be defined as \n",
    "\n",
    ">Classify ${\\bf x}$ as + if <br>\n",
    ">$p(+ ~|~ {\\bf x} = [blue,~ square]) \\geq p(- ~|~ {\\bf x} = [blue, ~square])$ <br>\n",
    ">else classify sample as -\n",
    "\n",
    "**B**: What are the Maximum Likelihood Estimates of the priors $p(+)$ and $p(-)$?<br>\n",
    ">$p(+) = 7/12 \\\\ p(-) = 5/12$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C**: Identify and compute estimates of the class-conditional probabilities required to predict the class of ${\\bf x} = [blue,~square]$?\n",
    ">$p(blue|+) ~= ~3/7 \\\\ ~p(square|+) ~= ~5/7 \\\\ ~p(blue|-) ~= ~3/5 \\\\~p(square|-) ~= ~3/5$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D**: Using the estimates computed above, compute the **posterior** scores for each label, and find the Naive Bayes prediction of the label for ${\\bf x} = [blue,~square]$. <br>\n",
    "\n",
    ">$p(+|b,s) ~is ~proportional ~to ~7/12 * 3/7 * 5/7 ~= ~0.18$<br>\n",
    ">$p(-|b,s) ~is ~proportional ~to ~5/12 * 3/5 * 3/5 ~= ~0.15$<br>\n",
    "\n",
    "The naive bayes prediction is +"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E**: If you haven't already, compute the class-conditional probabilities scores $\\hat{p}({\\bf x} = [blue,~square] ~|~ +)$ and $\\hat{p}({\\bf x} = [blue,~square] ~|~ -)$ under the Naive Bayes assumption.  How can you reconsile these values with the final prediction that would made? \n",
    "<br><br>\n",
    "Note that if we were to take only class conditional probabilities into consideration, we would have predicted -, this is the way prior knowlege affects our computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prob3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Laplace Smoothing \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the same training set from Problem 2, but suppose we see the following test example: \n",
    "    \n",
    "<img src=\"figs/greencircle.png\" width=200>\n",
    "\n",
    "Before you get too far into trying to predict the label of the green circle, look carefully at the training set.  Notice that there are no green shapes labeled - in the training set, so when we try to compute the class-conditional probability $p(green ~|~ -)$ we'll get a zero probability.  To fix this, you'll implement Laplace smoothing. Notice that this is a little different than the SPAM vs HAM example shown in the video.  We actually have two very different features in shapes and colors. We'll apply Laplace Smoothing to the shape and color class-conditional probabilities separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: What would the general formula for the estimate of $p(shape ~|~ class)$ with Laplace Smoothing look like for the given training set?  What is the *vocabulary* in the shape case? <br>\n",
    ">$p(shape='shape' ~|~ class) = (~(~number ~of ~shapes ~= ~'shape'~in~class~) ~+ ~1)~~/~~(~total~number~of~figures~in~class~ ~+~~number~of~distinct~shapes~in~class~)$\n",
    "\n",
    "Vocabulary of shape: SQUARE, CIRCLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B**: What would the general formula for the estimate of $p(color ~|~ class)$ with Laplace Smoothing look like for the given training set?  What is the *vocabulary* in the color case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$p(color='color' ~|~ class) =  = (~(~number ~of ~colors ~= ~'color'~in~class~) ~+ ~1)~~/~~(~total~number~of~figures~in~class~ ~+~~number~of~distinct~colors~in~class~)$\n",
    "\n",
    "Vocabulary of color: RED, GREEN, BLUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C**: Predict the label for the green circle using the Laplaced smoothed class-conditional probability formulas.  Don't forget to apply Laplace Smoothing to the priors as well! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$p(green|+) ~= ~(2+1)/(7+3) ~= 3/10\\\\\n",
    "~p(circle|+) ~= ~(2+1)/(7+2) ~= 1/3\\\\\n",
    "~p(green|-) ~= ~(0+1)/(5+3) ~= 1/8\\\\\n",
    "~p(circle|-) ~= ~(2+1)/(5+2) ~= 3/7\\\\\n",
    "~p(+) ~= ~(7+1)/(12+2) ~= 4/7\\\\\n",
    "~p(-) ~= ~(5+1)/(12+2) ~= 3/7$<br>\n",
    "\n",
    "Predicting label for green circle:<br>\n",
    ">$p(+|green,circle) = p(+) * p(green|+) * p(circle|+) = 4/7 * 3/10 * 1/3 = 2/35 = 0.05714$\n",
    ">$p(-|green,circle) = p(-) * p(green|-) * p(circle|-) = 3/7 * 1/8 * 3/7 = 9/392 = 0.02296$\n",
    "<br>\n",
    "\n",
    "Predicted label is +"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Unknown Features\n",
    "***\n",
    "\n",
    "Once again consider the training set from Problem 2, but suppose we see the following test example: \n",
    "    \n",
    "<img src=\"figs/yellowsquare.png\" width=200>\n",
    "\n",
    "OK, this is a weird one.  Up until this point, we've never seen the color *yellow*, and thus don't include it in the color vocabulary.  One way that we could handle this is to add to the color vocabulary, and then recompute the the class-conditional probabilities with *yellow* included in the vocabulary. \n",
    "\n",
    "But what happens when on the next test example we see a *pink* circle (or worse, a triangle)? We'd rather not continue to modify our probability estimates whenever we see shape or color that we haven't see before.  One solution to this is to just assume we'll see weird things in the future and combine all of the posibilities into an UNK feature. If we do this, then our class-conditional probabilities become \n",
    "\n",
    "$$\n",
    "p(feature ~|~ class) = \\frac{\\#~instances~of~feature~in~class + 1}{\\#~total~symbols~in~class + |V| + 1}\n",
    "$$\n",
    "\n",
    "where here the vocabular $V$ is the same vocabular defined by the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Predict the label of the yellow square.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$p(yellow|+) ~= ~(0+1)/(7+3+1) ~= 1/11\\\\\n",
    "~p(square|+) ~= ~(5+1)/(7+2+1) ~= 3/5\\\\\n",
    "~p(yellow|-) ~= ~(0+1)/(5+3+1) ~= 1/9\\\\\n",
    "~p(square|-) ~= ~(3+1)/(5+2+1) ~= 1/2\\\\\n",
    "~p(+) ~= ~(7+1)/(12+2) ~= 4/7\\\\\n",
    "~p(-) ~= ~(5+1)/(12+2) ~= 3/7$<br>\n",
    "\n",
    "Predicting label for yellow square:<br>\n",
    ">$p(+|yellow,square) = p(+) * p(yellow|+) * p(square|+) = 4/7 * 1/11 * 3/5 = 2/35 = 0.03116$\n",
    ">$p(-|green,circle) = p(-) * p(yellow|-) * p(square|-) = 3/7 * 1/9 * 1/2 = 9/392 = 0.0238$\n",
    "<br>\n",
    "\n",
    "Predicted label is +"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prob1ans'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "### Helper Functions \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
